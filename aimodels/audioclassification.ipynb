{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-06T01:00:04.502266900Z",
     "start_time": "2024-03-06T01:00:04.123292500Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login as notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\lewis\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "notebook_login(token=\"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\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T01:00:07.478300Z",
     "start_time": "2024-03-06T01:00:06.830464700Z"
    }
   },
   "id": "5bb27bbbad967a35",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lewis\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\datasets\\load.py:1461: FutureWarning: The repository for agkphysics/AudioSet contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/agkphysics/AudioSet\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"agkphysics/AudioSet\", split=\"train[:14000]\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T01:00:14.492236200Z",
     "start_time": "2024-03-06T01:00:10.687105700Z"
    }
   },
   "id": "497091204a158e81",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "len(dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ff85ca88bf45fb1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/14000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59843fb39c2848e3b163da7593f3e421"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define your categories of interest\n",
    "categories = [\"Breathing\", \"Snoring\", \"Cough\", \"Fart\", \"Sneeze\", \"Whispering\", \"Other\"]  # Example categories\n",
    "\n",
    "\n",
    "# Function to process and filter the dataset\n",
    "def process_dataset(example):\n",
    "    # Initialize label as \"Other\"\n",
    "    label = \"Other\"\n",
    "    # Check if human_labels intersect with categories of interest\n",
    "    for human_label in example[\"human_labels\"]:\n",
    "        if human_label in categories:\n",
    "            label = human_label\n",
    "            break\n",
    "    # Convert label to a numerical format (e.g., 0 for Breathing, 1 for Snoring, 2 for Other)\n",
    "    label_id = categories.index(label)\n",
    "    example[\"label\"] = label_id\n",
    "    return example\n",
    "\n",
    "\n",
    "# Apply the processing function to the dataset\n",
    "processed_dataset = dataset.map(process_dataset)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T01:01:20.016131100Z",
     "start_time": "2024-03-06T01:00:24.237969200Z"
    }
   },
   "id": "34dcc1bf8344d0c9",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lewis\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:365: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'video_id': '--PJHxphWEs',\n 'audio': {'path': 'audio/bal_train/--PJHxphWEs.flac',\n  'array': array([-0.04364824, -0.05268681, -0.0568949 , ...,  0.11446512,\n          0.14912748,  0.13409865]),\n  'sampling_rate': 48000},\n 'labels': ['/m/09x0r', '/t/dd00088'],\n 'human_labels': ['Speech', 'Gush'],\n 'label': 6}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "processed_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T01:02:15.546113700Z",
     "start_time": "2024-03-06T01:02:09.896089500Z"
    }
   },
   "id": "289ec2b1387e8a3c",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Collect unique labels\n",
    "unique_labels = set()\n",
    "for example in processed_dataset:\n",
    "    for label in example['human_labels']:\n",
    "        unique_labels.add(label)\n",
    "\n",
    "# Create mappings\n",
    "label2id = {label: id for id, label in enumerate(unique_labels)}\n",
    "id2label = {id: label for label, id in label2id.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T01:11:18.047602800Z",
     "start_time": "2024-03-06T01:08:19.237879200Z"
    }
   },
   "id": "a66cb9975e17c997",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T01:11:57.154419400Z",
     "start_time": "2024-03-06T01:11:57.085597400Z"
    }
   },
   "id": "b90d3c078bb3c919",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "\n",
    "def resample_audio(batch):\n",
    "    audio_array = np.array(batch['audio']['array'])  # Adjust according to your dataset structure\n",
    "    original_sr = batch['audio']['sampling_rate']  # Adjust according to your dataset structure\n",
    "    target_sr = 16000  # Target sampling rate\n",
    "\n",
    "    # Resample audio\n",
    "    batch['audio']['array'] = librosa.resample(audio_array, orig_sr=original_sr, target_sr=target_sr)\n",
    "    batch['audio']['sampling_rate'] = target_sr  # Update the sampling rate in the dataset\n",
    "    return batch\n",
    "\n",
    "\n",
    "# Apply the function to resample audio in the dataset\n",
    "resampled_dataset = dataset.map(resample_audio)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b98d6613d3759db",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/14000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "25aebad000f24e83b66670d528a11e3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "processor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "\n",
    "def preprocess_audio(examples):\n",
    "    audio_arrays = [audio['array'] for audio in examples[\"audio\"]]\n",
    "\n",
    "    inputs = processor(audio_arrays, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Assuming `dataset` is your dataset loaded and contains an 'audio' field as described\n",
    "processed_dataset = dataset.map(preprocess_audio, batched=True, remove_columns=['audio'])\n",
    "# Note: Setting `batched=False` because we are now iterating over each example individually.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T01:52:45.236186700Z",
     "start_time": "2024-03-06T01:47:53.515625600Z"
    }
   },
   "id": "c4ec770be8820ce4",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\transformers\\feature_extraction_utils.py:182\u001B[0m, in \u001B[0;36mBatchFeature.convert_to_tensors\u001B[1;34m(self, tensor_type)\u001B[0m\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tensor(value):\n\u001B[1;32m--> 182\u001B[0m     tensor \u001B[38;5;241m=\u001B[39m \u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;28mself\u001B[39m[key] \u001B[38;5;241m=\u001B[39m tensor\n",
      "File \u001B[1;32m~\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\transformers\\feature_extraction_utils.py:140\u001B[0m, in \u001B[0;36mBatchFeature._get_is_as_tensor_fns.<locals>.as_tensor\u001B[1;34m(value)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(value) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value[\u001B[38;5;241m0\u001B[39m], np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[1;32m--> 140\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mtensor(value)\n",
      "\u001B[1;31mValueError\u001B[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8,) + inhomogeneous part.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 24\u001B[0m\n\u001B[0;32m     14\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForAudioClassification\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfacebook/wav2vec2-base\u001B[39m\u001B[38;5;124m\"\u001B[39m, num_labels\u001B[38;5;241m=\u001B[39mnum_labels, label2id\u001B[38;5;241m=\u001B[39mlabel2id, id2label\u001B[38;5;241m=\u001B[39mid2label\n\u001B[0;32m     16\u001B[0m )\n\u001B[0;32m     17\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     18\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     19\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m     20\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mprocessed_dataset,  \u001B[38;5;66;03m# Make sure you have a train split\u001B[39;00m\n\u001B[0;32m     21\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mprocessor,\n\u001B[0;32m     22\u001B[0m )\n\u001B[1;32m---> 24\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\transformers\\trainer.py:1624\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1622\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   1623\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1624\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1625\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1626\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1627\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1628\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1629\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\transformers\\trainer.py:1928\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1925\u001B[0m     rng_to_sync \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1927\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1928\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, inputs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(epoch_iterator):\n\u001B[0;32m   1929\u001B[0m     total_batched_samples \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1931\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39minclude_num_input_tokens_seen:\n",
      "File \u001B[1;32m~\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\accelerate\\data_loader.py:452\u001B[0m, in \u001B[0;36mDataLoaderShard.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001B[39;00m\n\u001B[0;32m    451\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 452\u001B[0m     current_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    454\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\transformers\\data\\data_collator.py:271\u001B[0m, in \u001B[0;36mDataCollatorWithPadding.__call__\u001B[1;34m(self, features)\u001B[0m\n\u001B[0;32m    270\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, features: List[Dict[\u001B[38;5;28mstr\u001B[39m, Any]]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[1;32m--> 271\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[43mpad_without_fast_tokenizer_warning\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    276\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    277\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    278\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    279\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m batch:\n\u001B[0;32m    280\u001B[0m         batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\transformers\\data\\data_collator.py:59\u001B[0m, in \u001B[0;36mpad_without_fast_tokenizer_warning\u001B[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# To avoid errors when using Feature extractors\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(tokenizer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdeprecation_warnings\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;241m*\u001B[39mpad_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpad_kwargs)\n\u001B[0;32m     61\u001B[0m \u001B[38;5;66;03m# Save the state of the warning, then disable it\u001B[39;00m\n\u001B[0;32m     62\u001B[0m warning_state \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdeprecation_warnings\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsking-to-pad-a-fast-tokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\transformers\\feature_extraction_sequence_utils.py:224\u001B[0m, in \u001B[0;36mSequenceFeatureExtractor.pad\u001B[1;34m(self, processed_features, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors)\u001B[0m\n\u001B[0;32m    221\u001B[0m             value \u001B[38;5;241m=\u001B[39m value\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m    222\u001B[0m         batch_outputs[key]\u001B[38;5;241m.\u001B[39mappend(value)\n\u001B[1;32m--> 224\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mBatchFeature\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\transformers\\feature_extraction_utils.py:78\u001B[0m, in \u001B[0;36mBatchFeature.__init__\u001B[1;34m(self, data, tensor_type)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, data: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, Any]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, tensor_type: Union[\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mstr\u001B[39m, TensorType] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     77\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(data)\n\u001B[1;32m---> 78\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_type\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\yukaclone\\.venv\\lib\\site-packages\\transformers\\feature_extraction_utils.py:188\u001B[0m, in \u001B[0;36mBatchFeature.convert_to_tensors\u001B[1;34m(self, tensor_type)\u001B[0m\n\u001B[0;32m    186\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverflowing_values\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    187\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to create tensor returning overflowing values of different lengths. \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 188\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    189\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to create tensor, you should probably activate padding \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    190\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwith \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpadding=True\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to have batched tensors with the same length.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    191\u001B[0m         )\n\u001B[0;32m    193\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[1;31mValueError\u001B[0m: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2Processor, TrainingArguments, Trainer, \\\n",
    "    AutoModelForAudioClassification\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "num_labels = len(id2label)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\", num_labels=num_labels, label2id=label2id, id2label=id2label\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,  # Make sure you have a train split\n",
    "    tokenizer=processor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T02:19:38.403938300Z",
     "start_time": "2024-03-06T02:19:36.561786400Z"
    }
   },
   "id": "daabd5e3496400d",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "79b3d6f20af9ddb1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
